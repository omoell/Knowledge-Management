{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear vs. non-linear regressions:\n",
    " - Inspect visually (input vs. output)\n",
    " - Calculate corellation coefficients, if for all parameters > 0.7 linear tendency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regressions\n",
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = pd.DataFrame({'Enginesize': [2.0,2.4,1.5,3.5,3.5,3.5,3.5,3.7,3.7], 'CO2 Emissions': [196,221,136,255,244,230,232,255,267]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error:\n",
    "Sum of squares of the distance from the predicted value $\\widehat{y}$ to the true value $y$.\n",
    "\n",
    "$\\widehat{y}=\\theta_0+\\theta_1x_1$\n",
    "\n",
    "$MSE=\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\widehat{y_i} \\right)^2$\n",
    "\n",
    "<br>How to find best values for $\\theta_0$ and $\\theta_1$?\n",
    "Two options available:  \n",
    "1) Mathematic approach  (see simple linear regression) -> Ordinary Least Squares  \n",
    "2) Optimization approach  (see multiple linear regression) -> Gradient Descent  \n",
    "\n",
    "Optimization used best for multiple linear regression with large datasets, as calculations required lower than for matrix calculations of mathematical approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematic approach to simple linear regression\n",
    "\n",
    "Given the condition, that we have a simple linear regression, i.e. only two paramaters.  \n",
    "Calculations for both parameters:\n",
    "\n",
    "$\\theta_1=\\frac{\\sum_{i=1}^s(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^s(x_i-\\overline{x})^2} $\n",
    "\n",
    "$ \\theta_0=\\overline{y}-\\theta_1\\overline{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.299999999999997"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = example_data['Enginesize']\n",
    "y = example_data['CO2 Emissions']\n",
    "\n",
    "# Calculate means of x and y\n",
    "mean_x = sum(x) / len(x)\n",
    "mean_y = sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient (theta_1): 43.98446833930704\n"
     ]
    }
   ],
   "source": [
    "# Calculate fraction for theta_1\n",
    "numerator = sum([(x[i]-mean_x)*(y[i]-mean_y) for i in range(len(x))])\n",
    "denominator = sum([(x[i]-mean_x)**2 for i in range(len(x))])\n",
    "\n",
    "theta_1 = numerator / denominator\n",
    "print(f'Coefficient (theta_1): {theta_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (theta_0): 92.80266825965754\n"
     ]
    }
   ],
   "source": [
    "# Input theta_1 to calculate theta_0\n",
    "theta_0 = mean_y - (theta_1*mean_x)\n",
    "print(f'Intercept (theta_0): {theta_0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\widehat{y} = 92.80 + 43.98x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics in Regression Models\n",
    "\n",
    "**Mean Absolute Error:**  \n",
    "<br>$ MAE=\\frac{1}{n}\\sum_{j=1}^n|y_j-\\widehat{y_j}| $\n",
    "\n",
    "\n",
    "<br>**Mean Squared Error:**  \n",
    "MSE is more popular than MAE, as it is more geared towards large errors.  \n",
    "<br>$ MSE = \\frac{1}{n}\\sum_{i=1}^n(y_j-\\widehat{y_j})^2 $\n",
    "\n",
    "\n",
    "\n",
    "<br>**Root Mean Squared Error:**  \n",
    "RMSE is one of the most popular evaluation metric, as it is interpretable in the same units as the response vector or y units.  \n",
    "<br>$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_j-\\widehat{y_j})^2} $\n",
    "\n",
    "\n",
    "\n",
    "<br>**Relative Absolute Error:**  \n",
    "<br>$ RAE=\\frac{\\sum_{j=1}^n|y_j-\\widehat{y_j}|}{\\sum_{j=1}^n|y_j-\\overline{y}|} $\n",
    "\n",
    "\n",
    "<br>**Relative Squared Error:**  \n",
    "RSE is widely adopted by Data Science community, due to being used for calculating $R^2$.  \n",
    "<br>$ RSE=\\frac{\\sum_{j=1}^n(y_j-\\widehat{y_j})^2}{\\sum_{j=1}^n(y_j-\\overline{y})^2} $\n",
    "\n",
    "\n",
    "<br>**$R^2:$**  \n",
    "<br>$ R^2=1-RSE $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of results with `sklearn.linear_model.LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: 43.984468339307035 \n",
      "Intercept: 92.80266825965757\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "lr = LinearRegression()\n",
    "lr.fit(pd.DataFrame(x),y)\n",
    "print(f'Coefficient: {lr.coef_[0]} \\nIntercept: {lr.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Multiple Linear Regression normall used for two differente purposes:\n",
    " - Understand impact of independent variables on prediction\n",
    " - Understand and predict impact of change on predictor (dependent variable), if any variable (independent variable) changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mlr = pd.DataFrame({\n",
    "                                                    'Enginesize': [2.0,2.4,1.5,3.5,3.5,3.5,3.5,3.7,3.7],\n",
    "                                                    'Cylinders': [4,4,4,6,6,6,6,6,6],\n",
    "                                                    'Fuelconsumption_Comb': [8.5,9.6,5.9,11.1,10.6,10.0,10.1,11.1,11.6],\n",
    "                                                    'CO2 Emissions': [196,221,136,255,244,230,232,255,267]\n",
    "                                                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target function:\n",
    "\n",
    "$\\widehat{y}=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$\n",
    "\n",
    "Function can be written as:\n",
    "$ \\widehat{y}=\\theta^TX $\n",
    "\n",
    "$ \\theta^T=[\\theta_0,\\theta_1,\\theta_2,...] $ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ X=\\left(\\begin{array}{c}1\\\\x_1\\\\x_2\\\\...\\\\\\end{array}\\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Regressions\n",
    "\n",
    "$ \\widehat{y} $ must be a non-linear function of the paramaters $ \\theta $, not neccessarily the features x. To cope with non-linear data, use polynomial regression, non-linear regression model or data transformation.\n",
    "\n",
    "Examples for non-linear functions:\n",
    "\n",
    "$ \\widehat{y}=\\theta_0+\\theta_2^2x $  \n",
    "\n",
    "$ \\widehat{y}=\\theta_0+\\theta_1\\theta_2^x $  \n",
    "\n",
    "$ \\widehat{y}=\\log{(\\theta_0+\\theta_1x+\\theta_2x^2+\\theta_3x^3)} $  \n",
    "\n",
    "$ \\widehat{y}=\\frac{\\theta_0}{1+\\theta_1^(x-\\theta_2)} $  \n",
    "\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "Polynomial Regression comprises all regressions, for which the relationship between the independent variable x and the dependent variable y is modeled as an $n^{th}$ degree polynomial in x. Thus Polynomial Regression fits a curved line to the data, e.g.:\n",
    "\n",
    "$ \\widehat{y}=\\theta_0+\\theta_1x+\\theta_2x^2+\\theta_3x^3 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook for Coursera course (ML0101EN-Reg-NoneLinearRegression-py-v1.ipynb) !!!"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
